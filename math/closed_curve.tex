\documentclass[a4paper,11pt]{article}
\usepackage[scale=0.7,vmarginratio={1:2},heightrounded]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{tensor}
\usepackage{amsfonts}
\usepackage{pgf,tikz}
\usepackage{pgfplots}
\usepackage{enumitem}
%\usepackage[margin=0.5in]{geometry}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\theoremstyle{definition}
\newtheorem*{observation}{Observation}
\newtheorem*{example}{Example}
\newtheorem*{remark}{Remark}
\newtheorem{definition}[theorem]{Definition} 

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\M}{\mathbb{M}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\J}{\mathbf{J}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\dx}{\text{d}}
\newcommand{\D}{\text{D}}
\newcommand{\del}{\partial}
\newcommand{\iff}{\Longleftrightarrow}
\newcommand{\dd}{\text{..}}
\newcommand{\lett}{\sqsupset}
\DeclareMathOperator{\grad}{grad}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\dive}{div}
\DeclareMathOperator{\curl}{curl}
\newcommand{\jhat}{\mathbf{j}}
\newcommand{\ihat}{\mathbf{i}}
\newcommand{\khat}{\mathbf{k}}


\title{Closed curves}
\author{Erik. S. Gimsing}
\date{January 2018}

\begin{document}
\maketitle
\begin{definition}[Simple closed curve]\label{def:simple_closed_curve}
	A simple closed curve $\Gamma$ is a subset of $\R^2$ with a parametrization
	$\gamma:[0,1]\to\R^2$ such that $\gamma$ is injective on $(0,1)$, continues
	and $\gamma(0)=\gamma(1)$. That is to say $\im(\gamma)=\Gamma$.
\end{definition}
\begin{definition}[Simple closed polygon]\label{def:simple_closed_polygon}
	Let $\Gamma$ be a polygon with vertexes $v_0,\dots,v_n\in\R^2$ and edges
	\begin{align*}
		E_i &= \{tv_i+(1-t)v_{i+1}:t\in[0,1]\} \\
		E_n &= \{tv_n+(1-t)v_0:t\in[0,1]\}
	\end{align*}
	Then $\Gamma$ is called a simple closed polygon if $E_i\cap
	E_{i+1}=\{v_{i+1}\}$ and $E_i\cap E_j = \emptyset$ otherwise.
\end{definition}
\begin{proposition}[Simple closed polygon is simple closed
	curve]\label{prop:polygon_is_curve}
	Let $\Gamma$ be a simple closed curve, there is a continues function
	$\gamma:[0,1]\to\R^2$ making $\Gamma$ a simple closed curve.
\end{proposition}
\begin{proof}
	Let $\Gamma$ be an $n+1$-sided polygon. We may define the function
	as follows: For each $E_i$ there is an obvious continues function
	$e_i:[0,1]\to\R^2$ with $\im(e_i) = E_i$ defined by
	$e_i(t)=(1-t)v_i+tv_{i+1}$. Using those we may define
	$\gamma:[0,n+1]\to\R^2$
	\begin{align*}
		\gamma(t) =
		\begin{cases}
			e_i(t-i), & i < t \leq i+1 \\
			e_n(t-n), & n < t \leq n+1
		\end{cases}
	\end{align*}
	Also since $E_i\cap E_{i+1} = v_i$ we have that
	\begin{align*}
		\lim_{t\to i+1} \gamma(t) = e_i(1) = e_{i+1}(0)
	\end{align*}
	Meaning that $\gamma$ is continues, since $E_i\cap E_j = \emptyset$
	and $E_i\cap E_{i+1}=\{v_{i+1}\}$ it is injective on
	$(0,1)$ and $\gamma(0)=\gamma(n+1)$. Thus clearly $\gamma(t(n+1))$
	satisfies definition~\ref{def:simple_closed_curve}.
\end{proof}
\begin{conjecture}[Simple closed curve theorem]
	Let $\Gamma$ be a simple closed curve. Then $\R^2/\Gamma$ consists of two,
	disjoint subsets $I,O\subseteq\R^2$ such that $I\cup O =\R^2/\Gamma$ and
	$I\cap O = \emptyset$. This property is called the simple closed curve
	property, or SCSP for short.
\end{conjecture}
\begin{lemma}[Simple closed polygon has SCSP]
\end{lemma}

\newpage
\begin{align*}
	d(t)&=re^{\omega i t} \\
	\\
	d'(t)=v(t)&=i\omega  r e^{\omega i t} \\
	\left|v(t)\right|=u&=\left|i\omega  r e^{\omega i t}\right| \\
	&=\omega \left|r e^{\omega i t}\right|= \omega r \\
	\implies \omega &= \frac{u}{r} \\
	\\
	v'(t)=a(t)&=-\omega^2  r e^{\omega i t} \\
	\left|a(t)\right| &= \left|-\omega^2  r e^{\omega i t}\right| \\
	&= \omega^2 \left|r e^{\omega i t}\right| \\
	&= \omega^2r = \frac{u^2}{r} \\
\end{align*}
\newpage
\begin{align*}
	D_x^n x^k &= \frac{k!\cdot x^{k-n}}{(k-n)!} \\
	D_x^\alpha x^k &= \frac{\Gamma(k+1)\cdot x^{k-\alpha}}
	{\Gamma(k-\alpha+1)} \\
	\\
	\text{Linearity}\\
	D_x^\alpha \lambda x^k &= \frac{\Gamma(k+1)\cdot \lambda x^{k-\alpha}}
	{\Gamma(k-\alpha+1)} \\
	&=\lambda D_x^\alpha x^k \\
	\\
	D_x^\alpha x^m+ D_x^\alpha x^k &:= D_x^\alpha (x^m+x^k)\\
	\\
	D_x^\alpha D_x^\beta x^k &= D_x^\alpha\left(
	\frac{\Gamma(k+1)\cdot x^{k-\beta}}{\Gamma(k-\beta+1)}\right) \\
	&=\frac{\Gamma(k+1)}{\Gamma(k-\beta+1)}
	D_x^\alpha x^{k-\beta}\\
	&=\frac{\Gamma(k+1)}{\Gamma(k-\beta+1)}
	\frac{\Gamma(k-\beta+1)\cdot x^{k-\beta-\alpha}}
	{\Gamma(k-\beta-\alpha+1)}\\
	&=\frac{\Gamma(k+1)\cdot x^{k-\beta-\alpha}}
	{\Gamma(k-\beta-\alpha+1)} = D_x^{\alpha + \beta}x^k
\end{align*}
\begin{align*}
	\sin(x+\pi/2) = \cos(x),&\ \cos(x+\pi/2) = -\sin (x),\\
	-\sin(x+\pi/2) = -\cos(x),&\ -\cos(x+\pi/2) = \sin(x) \\
	\\
	D_x^n \sin x &= \sin \left(x+\frac{\pi n}{2}\right) \\
	D_x^n \cos x &= \cos \left(x+\frac{\pi n}{2}\right) \\
	\\
	D_x^\alpha \sin x &= \sin \left(x+\frac{\pi \alpha}{2}\right) \\
	D_x^\alpha \cos x &= \cos \left(x+\frac{\pi \alpha}{2}\right) \\
	\\
	D_x^\alpha D_x^\beta \sin x 
	&= 	D_x^\alpha \sin \left(x+\frac{\pi \beta}{2}\right) \\
	&= 	\sin \left(x+\frac{\pi \beta}{2}+\frac{\pi \beta}{2}\right) \\
	&= 	\sin \left(x+\frac{\pi (\alpha+\beta)}{2}\right) 
	= D_x^{\alpha+\beta} \sin x\\
	\\
	D_x^\alpha D_x^\beta \cos x 
	&= 	D_x^\alpha \cos \left(x+\frac{\pi \beta}{2}\right) \\
	&= 	\cos \left(x+\frac{\pi \beta}{2}+\frac{\pi \beta}{2}\right) \\
	&= 	\cos \left(x+\frac{\pi (\alpha+\beta)}{2}\right) 
	= D_x^{\alpha+\beta} \cos x\\
\end{align*}
\begin{align*}
	D_x^n e^{\lambda x} &= \lambda^n e^{\lambda x} \\
	D_x^\alpha e^{\lambda x} &= \lambda^\alpha e^{\lambda x} \\
	\\
	D_x^\alpha D_x^\beta e^{\lambda x} &= D_x^\alpha\left( \lambda^\beta
	e^{\lambda x}\right) \\
	&= \lambda^\beta D_x^\alpha e^{\lambda x} \\
	&= \lambda^\beta \lambda^\alpha e^{\lambda x} 
	= \lambda^{\alpha+\beta} e^{\lambda x}\\
\end{align*}
\newpage
\begin{align*}
	mv' &= mg - kv^2 \\
	\frac{1}{g} v' &= 1 - \frac{k}{mg}v^2 \\
	\frac{1}{g}\frac{1}{1 - \frac{k}{mg}v^2} v' &= 1 \\
	\frac{1}{g}\int \frac{1}{1 - \frac{k}{mg}v^2} v'\ \text{d}t &= t+C \\
	\frac{1}{g}\int \frac{\text{d}v}{1 - \frac{k}{mg}v^2}  &= t+C \\
	\text{Let } u^2 = \frac{k}{mg}v^2 &\implies \text{d}u = \sqrt{k/mg} \\
	\frac{1}{g}\int\frac{\sqrt{k/mg}}{\sqrt{k/mg}} \frac{\text{d}v}{1 - 
	\frac{k}{mg}v^2}  &= t+C \\
	\sqrt{gk/m}\int \frac{\text{d}u}{1-u^2}&=t+C \\
	\sqrt{gk/m} \tanh^{-1} u&=t+C \\
	\sqrt{gk/m} \tanh^{-1}{\left(v\sqrt{k/mg}\right)} &=t+C \\
	v(t) &= \sqrt{mg/k} \tanh{\left(t\sqrt{m/gk}\right)} 
\end{align*}
\begin{align*}
	J&=\int \frac{x^2}{x^3+4}\ \text{d}x \\
	\text{Let } u = x^3+4 &\implies \text{d}u = 3x^2\ \text{d}x \\
	J&=\int \frac{3}{3}\frac{x^2}{x^3+4}\ \text{d}x \\
	 &=\frac{1}{3}\int \frac{\text{d}u}{u} \\
	 &=\frac{1}{3} \ln{\left|x^3+4\right|}
\end{align*}
\begin{align*}
	J&=\int \sin^3 (x)\cos(x)\ \text{d}x \\
	\text{Let } u = \sin x &\implies \text{d}u = \cos x\ \text{d}x \\
	J&=\int u^3\ \text{d}u \\
	 &=\frac{u^4}{4} = \frac{\sin{(x)}^4}{4} \\
\end{align*}
\begin{align*}
	J&=\int \frac{\ln^2 x}{x}\ \text{d}x \\
	\text{Let } u = \ln x &\implies \text{d}u = \frac{1}{x}\ \text{d}x \\
	J&=\int u^2\ \text{d}u \\
	 &=\frac{u^3}{3} = \frac{\ln{(x)}^3}{3} \\
\end{align*}
\begin{align*}
	J&=\int x \cos{\left(5x^2\right)}\ \text{d}x \\
	\text{Let } u = 5x^2 &\implies \text{d}u = 10x\ \text{d}x \\
	J&=\frac{1}{10}\int \cos u\ \text{d}u \\
	 &= \frac{\sin{\left(5x^2\right)}}{10} 
\end{align*}
\begin{align*}
	J&=\int (2x+5){(x^2+5x)}^7\ \text{d}x \\
	\text{Let } u = x^2+5x &\implies \text{d}u = 2x+5\ \text{d}x \\
	J&=\int u^7\ \text{d}u \\
	 &=\frac{{\left(x^2+5x\right)}^8}{8} \\
\end{align*}
\begin{align*}
	J&=\int {(3-x)}^{10}\ \text{d}x \\
	\text{Let } u = 3-x &\implies \text{d}u = - \text{d}x \\
	J&=-\int u^{10}\ \text{d}u \\
	 &=\frac{{\left(3-x\right)}^{11}}{11} \\
\end{align*}
\begin{align*}
	J&=\int \sqrt{(7x-9)}\ \text{d}x \\
	\text{Let } u = 7x-9 &\implies \text{d}u = 7\ \text{d}x \\
	J&=\frac{1}{7}\int \sqrt{u}\ \text{d}u \\
	 &=\frac{{2\left(7x-9\right)}^{1.5}}{21} \\
\end{align*}
\begin{align*}
	J&=\int \frac{x^3}{\sqrt[3]{1+x^4}}\ \text{d}x \\
	\text{Let } u = 1+x^4 &\implies \text{d}u = 4x^3\ \text{d}x \\
	J&=\frac{1}{4}\int \frac{1}{\sqrt[3]{u}}\ \text{d}u \\
	 &=\frac{{2\left(1+x^4\right)}^{2/3}}{12} \\
\end{align*}
\begin{align*}
	J&=\int e^{5x+2}\ \text{d}x \\
	\text{Let } u = 5x+2 &\implies \text{d}u = 5\ \text{d}x \\
	J&=\frac{1}{5}\int e^u\ \text{d}u \\
	 &=\frac{1}{5}e^{5x+2} \\
\end{align*}
\begin{align*}
	J&=\int 4\cos 3x\ \text{d}x \\
	\text{Let } u = 3 &\implies \text{d}u = 3\ \text{d}x \\
	J&=\frac{4}{3}\int \cos u\ \text{d}u \\
	 &=\frac{4}{3}\sin 3x \\
\end{align*}
\begin{align*}
	J&=\int \frac{\sin(\ln x)}{x}\ \text{d}x \\
	\text{Let } u = \ln x &\implies \text{d}u = \frac{1}{x}\ \text{d}x \\
	J&=\int \sin u\ \text{d}u \\
	 &=-\cos(\ln x) \\
\end{align*}
\begin{align*}
	J&=\int \frac{3x+6}{x^2+4x-3}\ \text{d}x \\
	\text{Let } u = x^2+4x-3 &\implies \text{d}u = 2x+4\ \text{d}x \\
	J&=\frac{3}{2}\int \frac{1}{u}\ \text{d}u \\
	 &=\frac{3}{2}\ln \left|x^2+4x-3\right| \\
\end{align*}
\begin{align*}
	J&=\int x 3^{x^2+1}\ \text{d}x \\
	\text{Let } u = x^2+1 &\implies \text{d}u = 2x\ \text{d}x \\
	J&=\frac{1}{2}\int 3^u\ \text{d}u \\
	 &=\frac{3^{x^2+1}}{2\ln 3} \\
\end{align*}
\begin{align*}
	J&=\int \frac{3}{x\ln x}\ \text{d}x \\
	\text{Let } u = \ln x &\implies \text{d}u = \frac{1}{x}\ \text{d}x \\
	J&=3\int \frac{1}{u}\ \text{d}u \\
	 &=3 \ln \left|\ln |x|\right| \\
\end{align*}
\begin{align*}
	J&=\int \frac{\cos 5x}{e^{\sin 5x}}\ \text{d}x \\
	\text{Let } u = \sin 5x &\implies \text{d}u = 5\cos 5x\ \text{d}x \\
	J&=\frac{1}{5}\int e^{-u}\ \text{d}u \\
	 &=\frac{1}{5} e^{-\sin 5x} \\
\end{align*}
\begin{align*}
	J&=\int_{0}^{\sqrt{\pi}} x\sin x^2\ \text{d}x \\
	\text{Let } u = x^2 &\implies \text{d}u = 2x\ \text{d}x \\
	J &= \frac{1}{2}\int_0^{\pi} \sin u\ \text{d}u \\
	  &= \frac{1}{2} (-\cos \pi + \cos 0) = 1
\end{align*}
\begin{align*}
	J&=\int (x+3){(x-1)}^5\ \text{d}x \\
	\text{Let } u = x-1 &\implies \text{d}u = \ \text{d}x \\
	J&=\int (u+4)u^5\ \text{d}u \\
	 &= \int u^6 \ \text{d}u + \int 4u^5\ \text{d}u \\
	 &= \frac{{(x-1)}^7}{7} + \frac{4{(x-1)}^6}{6}
\end{align*}
\begin{align*}
	J&=\int x \sqrt{4-x}\ \text{d}x \\
	\text{Let } u = 4-x &\implies \text{d}u = - \text{d}x \\
	J&= -\int (4-u)\sqrt{u}\ \text{d}u \\
	 &= \int u^{3/2}\ \text{d}u - \int 4\sqrt{u} \ \text{d}u \\
	 &= \frac{2{(4-x)}^{5/2}}{5} - \frac{8{(4-x)}^{3/2}}{3}
\end{align*}
\begin{align*}
	J&=\int \frac{x+5}{2x+3}\ \text{d}x \\
	\text{Let } u = 2x+3 &\implies \text{d}u = 2\ \text{d}x \\
						 &\implies x = \frac{u-3}{2} \\
	J&= \frac{1}{2}\int \frac{u/2-3/2+5}{u}\ \text{d}u \\
	 &= \frac{u}{4} + \frac{7}{4}\int \frac{\text{d}u}{u}  \\
	 &= \frac{u}{4} + \frac{7}{4}\ln |u|  \\
	 &= \frac{2x+3}{4} + \frac{7}{4}\ln |2x+3|  \\
\end{align*}
\begin{align*}
	J&=\int \frac{x^2+4}{x+2}\ \text{d}x \\
	\text{Let } u = x+2 &\implies \text{d}u = \text{d}x \\
						&\implies x = u-2 \\
	J&= \int \frac{{(u-2)}^2+4}{u}\ \text{d}u \\
	 &= \int \frac{u^2-4u+8}{u}\ \text{d}u \\
	 &= \int u\ \text{d}u -4 \int\ \text{d}u+8\int\frac{\text{d}u}{u} \\
	 &= \frac{{(x+2)}^2}{2} -4x-8+8 \ln \left|x+2\right|
\end{align*}
\begin{align*}
	J&=\int \frac{{(3 +\ln x)}^2(2-\ln x)}{4x}\ \text{d}x \\
	\text{Let } u = 3+\ln x &\implies \text{d}u =\frac{1}{x} \ \text{d}x \\
						&\implies -\ln x = 3-u \\
	J&= \frac{1}{4}\int u^2(5-u)\ \text{d}u \\
	 &= \frac{5}{4}\int u^2\ \text{d}u -\frac{1}{4}\int u^3\ \text{d}u \\
	 &= \frac{5u^3}{12}-\frac{u^4}{16} =\frac{5{(3+\ln x)}^3}{12}
										-\frac{{(3+\ln x)}^4}{16}
\end{align*}
\begin{align*}
	J&=\int_0^9 \sqrt{4-\sqrt{x}}\ \text{d}x \\
	\text{Let } u = 4-\sqrt{x} &\implies \sqrt{x} =4-u \\
							   &\implies x = u^2-8u+16 \\
							   &\implies \text{d}x = 2u-8\ \text{d}u\\
	J&= \int_4^1 \sqrt{u}(2u-8)\ \text{d}u \\
	 &= 2\int_4^1 u^{3/2}\ \text{d}u - 8 \int_4^1 \sqrt{u} \ \text{d}u \\
	 &= \frac{4u^{5/2}}{5} \bigg|_4^1- \frac{16u^{3/2}}{3} \bigg|_4^1\\
	 &= \left(\frac{4\cdot 1^{5/2}}{5}-\frac{4\cdot 4^{5/2}}{5}\right)
	-\left(\frac{16\cdot 1^{3/2}}{3}-\frac{16\cdot 4^{3/2}}{3}\right)\\
	&= -\frac{124}{5} + \frac{112}{3} = \frac{188}{10}
\end{align*}
\begin{align*}
	\frac{\dx y}{\dx x} &= y(b-ay) \\
	\frac{1}{y(b-ay)}\frac{\dx y}{\dx x} &= 1 \\
	\int \frac{1}{y(b-ay)}\frac{\dx y}{\dx x}\ \dx x &= x \\
	J=\int \frac{1}{y(b-ay)}\ \dx y &= x \\
	\\
	\frac{1}{y(b-ay)} &= \frac{A}{y}+\frac{B}{b-ay} \\
					  &= \frac{A(b-ay)+By}{y(b-ay)}\\
					1 &= A(b-ay)+By \\
					  &= Ab - aAy+By, \text{ Let } A =\frac{1}{b} \\
					0 &= By-\frac{ay}{b} \implies B = \frac{a}{b} \\
	\\
	J&= \frac{1}{b}\int\frac{\dx y}{y}+\frac{a}{b}\int\frac{1}{b-ay}\ \dx y \\
	\text{Let } u = b-ay &\implies \dx u = -a\ \dx y \\
	J&= \frac{\ln y}{b}-\frac{1}{b}\int\frac{\dx u}{u} \\
	 &= \frac{\ln y}{b}-\frac{\ln b-ay}{b}\\
	\frac{\ln y}{b}-\frac{\ln( b-ay)}{b} &= x \\
	\ln y - \ln (b-ay) &= bx\\
	\ln {\left(\frac{y}{b-ay}\right)} &= bx \\
	\frac{y}{b-ay} &= e^{bx}\\
	\frac{b-ay}{y} &= e^{-bx}\\
	\frac{b}{y} &= a+e^{-bx}\\
	\frac{y}{b} &=\frac{1}{a+e^{-bx}}\\
	y &=\frac{b}{a+e^{-bx}}
\end{align*}
\begin{align*}
	\frac{\dx N}{\dx t} &= rN\left(1-\frac{N}{K}\right) \\
	\frac{1}{N\left(1-\frac{N}{K}\right)}\frac{\dx N}{\dx t} &= r \\
	\int\frac{1}{N\left(1-\frac{N}{K}\right)}\frac{\dx N}{\dx t}\ \dx x&=rt+C \\
	I = \int\frac{1}{N\left(1-\frac{N}{K}\right)}\ \dx N&=rt+C \\
	\frac{1}{N\left(1-\frac{N}{K}\right)}
	&=\frac{A}{N}+\frac{B}{1-\frac{N}{K}}\\
	&=\frac{A\left(1-\frac{N}{K}\right)+BN}{N\left(1-\frac{N}{K}\right)}\\
	1&=A-\frac{AN}{K}+BN, \text{ Let } A= 1\\
	0&=BN-\frac{N}{K} \implies B= \frac{1}{K} \\
	I&=\int \frac{\dx N}{N}+\frac{1}{K}\int \frac{1}{1-\frac{N}{K}}\ \dx N\\
	\text{Let } u = 1-\frac{N}{K}&\implies \dx u = -\frac{1}{K}\ \dx N\\
	I&= \ln N - \int \frac{\dx u}{u}\\
	 &= \ln N - \ln \left(1-\frac{N}{K}\right) \\
	\ln N - \ln \left(1-\frac{N}{K}\right) &= rt+C\\
	\ln \left(\frac{N}{1-\frac{N}{K}}\right) &= rt+C\\
	\frac{N}{1-\frac{N}{K}} &= Ce^{rt}\\
	\frac{1-\frac{N}{K}}{N} &= Ce^{-rt}\\
	\frac{1}{N} &=\frac{1}{K}+ Ce^{-rt}\\
	N(t) &=\frac{1}{\frac{1}{K}+ Ce^{-rt}}\\
	N_0 = N(0) = \frac{1}{\frac{1}{K}+C} &\implies C = \frac{1}{N_0}-\frac{1}{K}
\end{align*}
\begin{align*}
	J&=\int \frac{1}{1+e^x}\ \dx x\\
	 &=\int \frac{e^{-x}}{e^{-x}}\frac{1}{1+e^x}\ \dx x\\
	 &=\int \frac{e^{-x}}{1+e^{-x}}\ \dx x\\
	\text{Let } u=1+e^{-x} &\implies \dx u = -e^{-x}\ \dx x\\
	J&=-\int \frac{\dx u}{u}\\
	 &= -\ln u = -\ln \left(1+e^{-x}\right)
\end{align*}
\begin{align*}
	\int fg' = \int f'g+fg
\end{align*}
\begin{align*}
	J&=\int_0^\infty \left(xe^{1-x}
	-\lfloor x\rfloor e^{1-\lfloor x \rfloor}\right)\ \dx x\\
	&=\int_0^\infty xe^{1-x}\ \dx x-
	\int_0^\infty\lfloor x\rfloor e^{1-\lfloor x \rfloor}\ \dx x\\
	I_1&=\int_0^\infty\lfloor x\rfloor e^{1-\lfloor x \rfloor}\ \dx x\\
	&=\sum_{n=0}^\infty ne^{1-n}=e\sum_{n=1}^\infty ne^{-n}\\
	\frac{1}{e^1}+\frac{2}{e^2}&=\frac{e^1+2}{e^2}\\
	\frac{1}{e^1}+\frac{2}{e^2}+\frac{3}{e^3}&=\frac{e^2+2e^1+3}{e^3}\\
	\sum_{n=1}^\infty ne^{-n}&=\lim_{m\rightarrow\infty}
	\frac{1}{e^m}\sum_{n=0}^{m-1}(m-n)e^{n}\\
\end{align*}
\newpage
\begin{theorem}[Continues Image of Closed Interval Is Closed Interval]
	Let $I\subseteq \R$ be a set such that
	\begin{equation}
	\begin{aligned}
		\forall x,y\in I: \forall z\in R: (x<z<y\implies z\in I)
	\end{aligned}
	\end{equation}
	and let $f:I\rightarrow \R$ be a continues real function. Then the image of
	$f$ also satisfies the above.
\end{theorem}
\begin{proof}
\end{proof}
\begin{theorem}[Intermediate Value Theorem]\label{th1}
	Let $f$ be a real function which is continues on $[a,b]$ such that
	$f(a)<f(b)$. Then for $f(a)<k<f(b)$:
	\begin{equation}
	\begin{aligned}
		\exists c \in (a,b):f(c) =k
	\end{aligned}
	\end{equation}
\end{theorem}
\begin{proof}
	Let $A=\{x\in[a,b]:f(z)<k\}$. Then $A\neq\emptyset$ since
	$a\in A$ and $\sup A$ exists since $b$ is an upper bound. Let
	$c =\sup A$, we claim $f(c)=k$.  Since $c$ is the supremum,
	there is $\delta_0\in\R_{>0}$ such that for $0<c-a'<\delta_0$
	we have both $a'\in A$ and by the continuity of $f$
	\begin{equation}
	\begin{aligned}
		|f(a')-f(c)|<\varepsilon
	\end{aligned}
	\end{equation}
	for all $\varepsilon\in\R_{>0}$. Then 
	\begin{equation}
	\begin{aligned}
		|f(a')-f(c)|&<\varepsilon\\
		|f(c)-f(a')|&<\varepsilon\\
		\implies f(c)&<f(a')+\varepsilon\\
		\implies f(c)&<k+\varepsilon,\ \because f(a')<k
	\end{aligned}
	\end{equation}
	we may also choose $\delta_1\in\R_{>0}$ such that for $0<b'-c<\delta_1$ we
	have both $b'\notin A$ and
	\begin{equation}
	\begin{aligned}
		|f(b')-f(c)|&<\varepsilon\\
		|f(c)-f(b')|&<\varepsilon\\
		\implies f(b')-\varepsilon&<f(c)\\
		\implies k-\varepsilon&<f(c),\ \because k\leq f(b')
	\end{aligned}
	\end{equation}
	let $\delta=\min(\delta_0,\delta_1)$, then we have
	\begin{equation}
	\begin{aligned}
		0<|x-c|<\delta\implies |f(x)-k|<\varepsilon \iff \lim_{x\rightarrow
		c}f(x)=k
	\end{aligned}
	\end{equation}
	so by the continuity of $f$ we have $f(c)=k$.
\end{proof}
\begin{theorem}[Rolle's Theorem]
	Let $f$ be a real function which is continues on $[a,b]$ and differntiable
	on $(a,b)$ such that $f(a)=f(b)$. Then
	\begin{equation}
	\begin{aligned}
		\exists \xi \in (a,b):f'(\xi) =0
	\end{aligned}
	\end{equation}
\end{theorem}
\begin{proof}
	Proof sketch: Use that image of interval is interval, and that it has max
	and min. Then consider the case of a constant function and when not use that
	max and min have $f'=0$.
\end{proof}
\begin{theorem}[Mean Value Theorem]
	Let $f$ be a real function which is continues on $[a,b]$ and differntiable
	on $(a,b)$. Then:
	\begin{equation}
	\begin{aligned}
		\exists \xi \in (a,b):f'(\xi) =\frac{f(b)-f(a)}{b-a}
	\end{aligned}
	\end{equation}
\end{theorem}
\begin{proof}
	Let $g(x)=f(x)-\frac{f(b)-f(a)}{b-a}(x-a)$. Then $g$ is continues on
	$[a,b]$, and differntiable on $(a,b)$. Note that $g(a)=f(a)$ and
	$g(b)=f(a)$, therefore by Rolle's Theorem there is $\xi\in(a,b)$ such that
	\begin{equation}
	\begin{aligned}
		g'(\xi) &= 0\\
		f'(\xi)-\frac{f(b)-f(a)}{b-a} &= 0\\
		f'(\xi) &= \frac{f(b)-f(a)}{b-a}
	\end{aligned}
	\end{equation}
	Which completes the proof.
\end{proof}
\begin{theorem}[Cauchy Mean Value Theorem]
	Let $f$ and $g$ be real functions which are continues on $[a,b]$ and 
	differntiable on $(a,b)$. Then:
	\begin{equation}
	\begin{aligned}
		\exists \xi \in (a,b):(f(b)-f(a))g'(\xi)=(g(b)-g(a))f'(\xi)
	\end{aligned}
	\end{equation}
\end{theorem}
\begin{proof}
\end{proof}
\begin{theorem}[L'H\^{o}pital's Rule]
	Let $f$ and $g$ be real functions which are continues on $[a,b]$ and 
	differntiable on $(a,b)$. Let $g'(x)\neq 0$ for all $x\in(a,b)$ and suppose
	there is some $c\in(a,b)$ such that $f(c)=g(c)=0$, then
	\begin{equation}
	\begin{aligned}
		\lim_{x\rightarrow c}\frac{f(x)}{g(x)}
		=\lim_{x\rightarrow c}\frac{f'(x)}{g'(x)}
	\end{aligned}
	\end{equation}
\end{theorem}
\begin{proof}
\end{proof}
\begin{theorem}[Mean Value Theorem for Integrals]
	Let $f$ be a continues real function on the closed interval $[a,b]$. Then:
	\begin{equation}
	\begin{aligned}
		\exists k \in [a,b]: \int_a^b f(x) \dx x = f(k)(b-a)
	\end{aligned}
	\end{equation}
\end{theorem}
\begin{proof}
\end{proof}
\newpage
\begin{align*}
	f(x)=x^3-x^2-8x+1 &\implies f'(x)=3x^2-2x-8\\
	3x^2-2x-8 &= 0\\
	\therefore x &= -4/3,\ 2\\
	\therefore \max(f([-2,2]))&=\max(\{f(-4/3),f(2),f(-2)\})=f(-4/3)\\
	\land \min(f([-2,2]))&=\min(\{f(-4/3),f(2),f(-2)\})=f(2)
\end{align*}
\begin{align*}
	\lett a_1<\cdots<a_n\land \lett f(x)=\sum_{i=1}^n {(x-a_i)}^2\\
	\therefore \D_x f(x)=\sum_{i=1}^n \D_x{(x-a_i)}^2=
	\sum_{i=1}^n 2(x-a_i)\\
	\therefore 0=\sum_{i=1}^n 2(x'-a_i)=2nx'-\sum_{i=1}^n 2a_i\\
	\implies x'=\frac{1}{n}\sum_{i=1}^n a_i \implies \min f = f(x')\because
	0\leq {(x-a_i)}^2
\end{align*}
\begin{align*}
	&\lett a_1<\cdots<a_n\land \lett f(x)=\sum_{i=1}^n |x-a_i|\\
	\forall i<j:&x<a_i<a_j \implies 0<|x-a_i|+|x-a_j|=-2x+a_i+a_j\\
				&a_i\leq x'\leq a_j\implies0<|x-a_i|+|x-a_j|=a_j-a_i\\
				& a_i<a_j<x'' \implies 0<|x-a_i|+|x-a_j|=2x-a_i-a_j\\
	\therefore& a_j-a_i=2a_j-a_i-a_j<2x''-a_i-a_j\\
	\therefore& a_j-a_i=-2a_i+a_i+a_j<-2x+a_i+a_j\\
	\therefore\min f &= \min \sum_{i=1}^n |x-a_i| 
	= a_n-a_1+\min \sum_{i=2}^{n-1} |x-a_i|\\
	\therefore 2\mid n &\implies \min f= a_n-a_i+\cdots+a_{n/2+1}-a_{n/2-1}\\
	\therefore 2\nmid n &\implies \min f=a_n-a_i+\cdots+a_{n-\lfloor n/2\rfloor}
\end{align*}
\begin{align*}
	\lett 0<a \land \lett &f(x)=\frac{1}{1+|x|}+\frac{1}{1+|x-a|}\\
	\lett x< 0 &\implies f(x)=\frac{1}{1-x}+\frac{1}{1-x+a}\\&\implies
	f'(x)=\frac{1}{{(1-x)}^2}+\frac{1}{{(a+1-x)}^2}>0\\
	\lett 0<x< a &\implies f(x)=\frac{1}{1+x}+\frac{1}{1-x+a}\\&\implies
	f'(x)=-\frac{1}{{(1+x)}^2}+\frac{1}{{(a+1-x)}^2}\\
	&\therefore f'(x)=0\implies x=a/2\\
	\lett a< x &\implies f(x)=\frac{1}{1+x}+\frac{1}{1+x-a}\\&\implies
	f'(x)=-\frac{1}{{(1+x)}^2}-\frac{1}{{(x+1-a)}^2}<0\\
	\therefore \min f=\min \{f(a/2),f(a),f(0)\}&=\min\{f(a/2),f(0)\}\because
	f(0)=f(a)\\
	f(0)=1+\frac{1}{1+a}=\frac{a+2}{a+1}&\land f(a/2)=\frac{4}{2+a}\\
	0<a^2\iff 4a+4&<{(a+2)}^2\iff \frac{4}{2+a}<\frac{a+2}{a+1}\\
	\therefore \min f =\frac{a+2}{a+1}
\end{align*}
\newpage
\begin{align*}
	\cos 2\theta + i \sin 2\theta &= e^{2i\theta } \\
								  &= {\left(e^{i\theta }\right)}^2 \\
								  &= {\left(\cos \theta+i\sin \theta\right)}^2\\
								  &= \cos^2 \theta 
									 + 2i \sin \theta \cos \theta - \sin^2 \\
	\implies &\cos 2\theta = \cos^2 \theta - \sin^2 \theta \\
	\implies &\sin 2\theta = 2 \sin \theta \cos \theta \\
	\implies &\cos \theta = \frac{\sin 2\theta}{2\sin \theta}
\end{align*}
\begin{definition}
\begin{align*}
	U_n := \{z\in \C: z^n = 1\} \\
\end{align*}
\end{definition}
\begin{theorem}
\begin{align*}
	\forall n \in \N: \forall z\in \C:  
	(z^n = 1 \implies \exists k \in \N_n : z = e^{2i k\pi / n})
\end{align*}
\end{theorem}
\begin{proof}
\begin{align*}
	&P_1:=
	\forall n \in \N: \forall z\in \C: \exists U_n= \{z_1,\dots,z_n\} 
	\subseteq \C:
	\forall 1 \leq i \leq n: z_i^n = 1 
	&& \text{(FTA)} \\
	&\forall k \in \N_n: {\left(e^{2ik \pi /n}\right)}^n = e^{2\pi i k} = 1,\
	\therefore \exists \{c_1,\dots,c_n\} \subseteq
	\C,\ \because\left|\N_n\right|=n \\
	&\exists z\in U_n : \forall k \in \N_n: e^{2ik \pi /n} = z \implies
	\exists c \notin U_n: c^n = 1 \implies \bot\ \because P_1
\end{align*}
\end{proof}
\begin{theorem}
\begin{align*}
	(U_n,\cdot) \cong (\Z/n\Z,+)
\end{align*}
\end{theorem}
\begin{proof}
\begin{align*}
	\lett\varphi:\Z/n\Z \rightarrow U_n: \varphi(k)=e^{2ik \pi /n}\\
	\lett k,m\in\Z/n\Z\land\varphi(k+m)=e^{2i(k+m) \pi /n}
	=e^{2ik \pi /n}e^{2im \pi/n}
	=\varphi(k)\varphi(m)\\
	\lett k,m\in\Z/n\Z\land \varphi(k)=\varphi(m)\iff e^{2ik \pi /n}
	=e^{2im \pi/n}
	\iff e^{2i(k-m) \pi /n}=1\\
	\therefore k=m\ \because -n<k-m<n \land e^0=1\\
	\text{Theorem 10}\implies\forall \omega\in U_n:\exists 
	k\in\Z/n\Z:\varphi(k)=\omega\\
	\therefore \exists \gamma:U_n\rightarrow \Z/n\Z:\forall \omega\in U_n,k\in
	\Z/n\Z:(\gamma(\varphi(\omega))=\omega\land\varphi(\gamma(k))=k)
\end{align*}
\end{proof}
\begin{theorem}
	Let $f$ be continues on $[a,b]$ and $f(a)<f(b)$. Then $f$ is
	injective on $[a,b]$ if and only if $f$ is strictly increasing
	on $[a,b]$
\end{theorem}
\begin{proof}
	\textbf{Necessary condition} (Injective $\implies$ Strictly increasing)

	First we show that $f(a)<f(x)$ for all $x\in (a,b)$. Assume
	for contradiction that $f(x)<f(a)$, note that $f(x)<f(a)<f(b)$
	so per the Intermediate Value Theorem there is $\xi\in(x,b)$
	such that $f(\xi)=f(a)$, contradicting $f$ being injective. So
	therefore $f(a)\leq f(x)$, but $f$ is injective, so $f(a)<f(x)$.

	Next let $x\in(a,b]$, then $f(a)<f(x)$, and let $x'\in(a,x)$. Aiming
	for contradiction, assume that $f(x)<f(x')$. We have that
	$f(a)<f(x)<f(x')$, so therefore per the Intermediate Value Theorem
	there is $\xi\in(a,x')$ such that $f(\xi)=f(x)$,  contradicting
	$f$ being injective. So therefore $f(x')\leq f(x)$, but since $f$ is
	injective, it must be the case that $f(x')<f(x)$.

	\textbf{Sufficient condition} (Strictly increasing $\implies$ Injective)

	Let $x\neq x'$ and without loss of generality assume $x<x'$,
	then since $f$ is strictly increasing it must be the case that
	$f(x)<f(x')$. That is $f(x)\neq f(x')$, which completes the proof.
\end{proof}
\begin{corollary}
	Let $f$ be continues on $[a,b]$ and $f(b)<f(a)$. Then $f$ is
	injective on $[a,b]$ if and only if $f$ is strictly decreasing
	on $[a,b]$. To see why, note that for $-f$ the above Theorem applies.
\end{corollary}
\begin{corollary}
	Let $f$ be continues on $[a,b]$ and $f(b)\neq f(a)$. Then $f$
	is injective on $[a,b]$ if and only if $f$ is strictly increasing
	or decreasing on $[a,b]$.
\end{corollary}
\begin{theorem}
	Let $f$ be continues and even on $\R$. Then there is $N\in\R$ such that for
	all $x\in\R$ we have either $f(x)\leq N$ or $N\leq f(x)$.
\end{theorem}
\begin{proof}
	Assume first that $f$ is unbounded above. We note that since
	the continues image of a closed interval is bounded, that $f$
	cannot be unbounded near some $a\in\R$. Therefore it must be
	the case that
	\begin{equation}
	\begin{aligned}
		\lim_{x\rightarrow +\infty}f(x)=+\infty\text{ or }
		\lim_{x\rightarrow -\infty}f(x)=+\infty
	\end{aligned}
	\end{equation}
	Note that
	\begin{equation}
	\begin{aligned}
		\lim_{x\rightarrow -\infty}f(x)=\lim_{x\rightarrow \infty}f(-x)=
		\lim_{x\rightarrow \infty}f(x)
	\end{aligned}
	\end{equation}
	therefore $f$ must be bounded below. It is easy to see that a similar
	argument holds when $f$ is unbounded below, showing that $f$ is bounded
	above. Lastly if $f$ is bounded, there is nothing to show.
\end{proof}
\begin{align*}
	i^i&=e^{i\ln i}\\
	i&=e^{\pi i/2}\\
	\ln i&=\pi i/2\\
	i^i&=e^{-\pi/2}\\
	\\
	\sqrt{i}&=i^{1/2}\\
			&=e^{\pi i/4}\\
	\\
	e^{\ln\left(x^y\right)}=x^y&={\left(e^{\ln x}\right)}^y=e^{y\ln x}\\
	\therefore \ln\left(e^{\ln\left(x^y\right)}\right)
	&=\ln\left(e^{y\ln x}\right)\\
	\implies \ln\left(x^y\right)&=y\ln x
\end{align*}
\begin{align*}
	i^i&=e^{i\ln i}\\
	i&=e^{\pi i/2}\\
	\ln i&=\pi i/2\\
	i^i&=e^{-\pi/2}\\
	\\
	\sqrt{i}&=i^{1/2}\\
			&=e^{\pi i/4}\\
	\\
	e^{\ln\left(x^y\right)}=x^y&={\left(e^{\ln x}\right)}^y=e^{y\ln x}\\
	\therefore \ln\left(e^{\ln\left(x^y\right)}\right)
	&=\ln\left(e^{y\ln x}\right)\\
	\implies \ln\left(x^y\right)&=y\ln x
\end{align*}
\begin{definition}[Polynomial vector space]
	Given some polynomial 
	\begin{equation}
	\begin{aligned}	
		P(x) = a_n x^n+a_{n-1} x^{n-1}+\cdots+a_1x+a_0
	\end{aligned}	
	\end{equation}
	Where $a_i$ is non zero. We define the corresponding vector as follows
	\begin{equation}
	\begin{aligned}	
		\begin{pmatrix}
			a_0 \\
			a_1 \\
			\vdots \\
			a_n \\
			0 \\
			\vdots
		\end{pmatrix}
	\end{aligned}	
	\end{equation}
	Addition is defined in the usual way for polynomials and the same for
	multiplication by a scalar. It's easy then to see that it forms a vector
	space.
\end{definition}
\begin{equation}
\begin{aligned}	
	\begin{bmatrix}a & b\\c & d\end{bmatrix}\begin{pmatrix}x \\y\end{pmatrix}
					 &=
	\begin{bmatrix}
		a & b \\
		c & d
	\end{bmatrix}
	\left(
	x
	\begin{pmatrix}
		1 \\
		0
	\end{pmatrix}
	+
	y
	\begin{pmatrix}
		0 \\
		1
	\end{pmatrix}
	\right)\\
	&=
	x\begin{pmatrix}a \\c\end{pmatrix}
	+y\begin{pmatrix}b \\d\end{pmatrix}\\
	&=\begin{pmatrix}ax+by \\cx+dy\end{pmatrix}


\end{aligned}	
\end{equation}
\begin{equation}
\begin{aligned}	
	\mathcal{F}\{f(t)\}&:=\int_{-\infty}^\infty f(t)e^{-2\pi it\xi}\ \dx t\\
	\mathcal{L}\{f(t)\}&:=\int_0^\infty e^{-st}f(t)\ \dx t\\
	\mathcal{B}\{f(t)\}&:=\int_{-\infty}^{\infty}e^{-st}f(t)\ \dx t\\
	\mathcal{M}\{f(t)\}&:=\int_0^\infty t^{s-1}f(t)\ \dx t\\
\end{aligned}	
\end{equation}
\begin{equation}
\begin{aligned}	
	\mathcal{L}\{\cos (at)\} &= \int_0^\infty \cos(at)e^{-st}\ \dx t\\
	 					   &= \int_0^\infty \cos(at)e^{-st}\ \dx t\\
						   &=-\frac{1}{s}\cos(at)e^{-st}\big|_0^\infty
	-\frac{a}{s}\int_0^\infty \sin(at)e^{-st}\ \dx t\\
	&= \frac{1}{s}-\frac{a}{s}\left(-\frac{1}{s}\sin(at)e^{-st}\big|_0^\infty
+\frac{a}{s}\int_0^\infty \cos(at)e^{-st}\right)\\
&=\frac{1}{s}-\frac{a^2}{s^2}\mathcal{L}\{\cos (at)\}\\
\therefore \mathcal{L}\{\cos (at)\} &=\frac{1}{s}
					-\frac{a^2}{s^2}\mathcal{L}\{\cos (at)\}\\
\mathcal{L}\{\cos (at)\}\frac{s^2+a^2}{s^2} &=\frac{1}{s}\\		
\mathcal{L}\{\cos (at)\} &= \frac{s}{a^2+s^2}
\end{aligned}	
\end{equation}
\begin{equation}
\begin{aligned}	
	\mathcal{L}\{e^{at}\} &= \int_0^\infty e^{at} e^{-st}\ \dx t\\
						 &= \int_0^\infty e^{(a-s)t}\ \dx t\\
						 &= \frac{e^{(a-s)t}}{(a-s)}\bigg|_0^\infty
	= \frac{1}{s-a}
\end{aligned}	
\end{equation}
\begin{equation}
\begin{aligned}	
	\frac{\dx}{\dx t}\int_a^b f(x,t)\ \dx x = 
	\int_a^b \frac{\del f}{\del t}(x,t)\ \dx x
\end{aligned}	
\end{equation}
\begin{equation}
\begin{aligned}	
	\mathcal{L}\{tf(t)\} &= \int_0^\infty tf(t)e^{-st}\ \dx t\\
	\frac{\dx}{\dx s} \int_0^\infty f(t)e^{-st}\ \dx t
	&=\int_0^\infty \frac{\del}{\del s} f(t)e^{-st}\ \dx t\\
	&=-\int_0^\infty tf(t)e^{-st}\ \dx t\\
	\therefore \mathcal{L}\{tf(t)\}&=-\frac{\dx }{\dx s}\mathcal{L}\{f(t)\}
\end{aligned}	
\end{equation}
\begin{equation}
\begin{aligned}	
	\mathcal{L}\left\{\frac{f(t)}{t}\right\} 
	&= \int_0^\infty \frac{f(t)}{t}e^{-st}\ \dx t\\
	\lett tg(t) &= f(t)\\
	\mathcal{L}\{tg(t)\}&=-\frac{\dx }{\dx s}\mathcal{L}\{g(t)\}\\
	\therefore \mathcal{L}\{f(t)\}&=-\frac{\dx }{\dx s}
	\mathcal{L}\left\{\frac{f(t)}{t}\right\} \\
	\int_s^\infty \mathcal{L}\{f(t)\}\ \dx s
	&=-\mathcal{L}\left\{\frac{f(t)}{t}\right\} \bigg|^\infty_s\\
	&=\lim_{s\rightarrow\infty}\mathcal{L}\left\{\frac{f(t)}{t}\right\}
	+\mathcal{L}\left\{\frac{f(t)}{t}\right\}\\
	&=\mathcal{L}\left\{\frac{f(t)}{t}\right\}
\end{aligned}	
\end{equation}
\begin{align*}
	\cL\{t\} &= -\frac{\dx}{\dx s}\cL\{1\}\\
			 &= -\frac{\dx}{\dx s}\int_0^\infty e^{-st}\ \dx t\\
			 &= -\frac{\dx}{\dx s}{\left[-\frac{e^{-st}}{s}\right]}_0^\infty\\
			 &= -\frac{\dx}{\dx s}\frac{1}{s}=\frac{1}{s^2}\\
\end{align*}
\begin{align*}
	\cL\{t^{n-1}\} &= \frac{(n-1)!}{s^n}\\
	\cL\{t^n\} &= \cL\{tt^{n-1}\}\\
			   &= -\frac{\dx}{\dx s}\cL\{t^{n-1}\}\\
			   &= -\frac{\dx}{\dx s}\frac{(n-1)!}{s^n}\\
			   &= \frac{n!}{s^{n+1}}\\
\end{align*}
\begin{align*}
	\Gamma(z)&=\int_0^\infty x^{z-1}e^{-x}\ \dx x\\
	\Gamma(1)&=\int_0^\infty e^{-x}\ \dx x={\left[-e^{-x}\right]}_0^\infty=1\\
	\text{Assume }\Gamma(n-1)&=(n-2)!\\
	\Gamma(n)&=\int_0^\infty x^{n-1}e^{-x}\ \dx x\\
			 &={\left[-x^{n-1}e^{-x}\right]}_0^\infty
	+(n-1)\int_0^\infty x^{n-2}e^{-x}\ \dx x\\
	&=(n-1)\int_0^\infty x^{n-2}e^{-x}\ \dx x\\
	&=(n-1)!\\
	\\
	\Gamma'(z)&=\frac{\dx}{\dx z}\int_0^\infty x^{z-1}e^{-x}\ \dx x\\
	          &=\int_0^\infty \frac{\del}{\del z}x^{z-1}e^{-x}\ \dx x\\
			  &=\int_0^\infty \ln(x)e^{z-1}e^{-x}\ \dx x\\
\end{align*}
\newline
\begin{align*}
	I&=\int_0^\infty \frac{\sin t}{t}\ \dx t\\
	\cL\left\{\frac{\sin t}{t}\right\}(s)
	&=\int_0^\infty\frac{\sin t}{t}e^{-st}\ \dx t \\
	\therefore \cL\left\{\frac{\sin t}{t}\right\}(0)
	&=\int_0^\infty\frac{\sin t}{t}\ \dx t \\
	\cL\left\{\frac{\sin t}{t}\right\}(s)
	&=\int_s^\infty\cL\{\sin t\}(u)\ \dx u \\
	&=\int_s^\infty\frac{1}{1+u^2}\ \dx u \\
	&={\left[\arctan u\right]}_s^\infty \\
	&=\frac{\pi}{2}-\arctan s \\
	\therefore \int_0^\infty\frac{\sin t}{t}\ \dx t
	&=\cL\left\{\frac{\sin t}{t}\right\}(0)\\
	&=\frac{\pi}{2}
\end{align*}
\newline
\begin{align*}
	I&=\int_0^\infty \frac{\sin^2 x}{x^2(x^2+1)}\ \dx x\\
	\frac{A}{x^2}+\frac{B}{x^2+1}&=\frac{1}{x^2(x^2+1)}\\
	A+Ax^2+Bx^2&=1\implies B=-A\\
	\therefore A&=1\\
	I&=\int_0^\infty \frac{\sin^2 x}{x^2}\ \dx x
	-\int_0^\infty \frac{\sin^2 x}{x^2+1}\ \dx x\\
	J_1(a)&=\int_0^\infty \frac{\sin^2 (ax)}{x^2}\ \dx x\\
	J_1'(a)&=\int_0^\infty \frac{2\sin(ax)\cos(ax)}{x}\ \dx x\\
		   &=\int_0^\infty \frac{\sin(2ax)}{x}\ \dx x=\frac{\pi}{2}\\
	\int_0^a J_1'(a')\ \dx a'&=J_1(a)-J_1(0)=J_1(a)=\frac{\pi a}{2}\\
	\therefore J_1(1)=\frac{\pi}{2}&=\int_0^\infty\frac{\sin^2x}{x^2}\ \dx x\\
	I&=\frac{\pi}{2}
	-\int_0^\infty \frac{\sin^2 x}{x^2+1}\ \dx x\\
	J_2 &= \int_0^\infty \frac{\sin^2 x}{x^2+1}\ \dx x \\
		&= \frac{1}{2}\int_0^\infty \frac{1-\cos(2x)}{x^2+1}\ \dx x \\
		&= \frac{1}{2}\int_0^\infty \frac{1}{x^2+1}\ \dx x
		- \frac{1}{2}\int_0^\infty \frac{\cos(2x)}{x^2+1}\ \dx x \\
		&= \frac{\pi}{4}
		- \frac{1}{4}\int_0^\infty \frac{e^{2ix}+e^{-2ix}}{x^2+1}\ \dx x \\
\end{align*}
\begin{align*}
\end{align*}
\begin{align*}
	\int_{\del \Omega} \omega = \int_{\Omega} \dx \omega
\end{align*}
\begin{align*}
	\cos(a-b)+i\sin(a-b) &= e^{(a-b)i} \\
	&= e^{ai} e^{-bi} \\
	&= (\cos(a)+i\sin(a))(\cos(-b)+i\sin(-b)) \\
	&= (\cos(a)+i\sin(a))(\cos(b)-i\sin(b)) \\
	&= \cos(a)\cos(b)+\sin(a)\sin(b)+i\sin(a)\cos(b)-i\cos(a)\sin(b) \\
	\\
	\sin(i) = \frac{e^{i}-e^{-i}}{2i}
\end{align*}
\newpage
\section{Construction of $\N$}
\begin{definition}[Axioms of $\N$]\label{nat axioms}
	Let $\N$ be a set with the following properties
	\begin{align*}
		(P1)\; &:\quad \exists 0\in\N\\
		(P2)\; &:\quad \exists S:\N\rightarrow\N\\
		(P3)\; &:\quad \forall m,n\in\N &&: S(m)=S(n)\implies m=n\\
		(P4)\; &:\quad \forall n\in\N &&: S(n)\neq 0\\
		(P5)\; &:\quad \forall N\subseteq\N &&: 
		(0\in N\land(\forall n\in N:s(n)\in N))\implies N=\N\\
	\end{align*}
\end{definition}
\begin{theorem}\label{nat proof}
	There exists a set $\N$ satisfying the axioms of
	definition~\ref{nat axioms}.
\end{theorem}
\begin{proof}
	Let $0=\emptyset$ and let $S(n)=\{n\}\cup n$. We claim that the set
	\begin{align*}
		\N =\{n:n=0\lor\exists m\in\N:S(m)=n\}
	\end{align*}
	satisfies the axioms of definition~\ref{nat axioms}. First note
	that $(P1)$ and $(P2)$ are already satisfied. For $(P3)$ we let
	$m,n\in\N$. Then assume
	\begin{align*}
		S(m)&=S(n)\iff \{m\}\cup m=\{n\}\cup n
	\end{align*}
	thus we have $n\in S(m)$. If $n\in\{m\}$ then we are done, so
	assume $n\in m$. Then $m\neq n$ by the Axiom of Foundation. We
	have $m\in \{m\}$ so by the Axiom of Extension and Axiom of
	Unions we have $m\in S(n)$. Since $n\neq m$ we must have $m\in
	n$ contradicting the Axiom of Foundation. We therefore conclude
	that $n=m$. $(P4)$ is trivially satisfied by the Axiom of the
	Empty Set. For the last $(P5)$ we can see that $\forall n\in
	N:s(n)\in N$ implies that $\exists m\in N$ such that $S(m)=n$
	for all $n \in N$. But that is just the definition of $\N$
	which completes the proof.
\end{proof}
\begin{theorem}[Proof by induction]\label{induction}
	Let $p(n)$ be a proposition defined for all $n\in \N$. If
	\begin{align*}
		p(0) \land p(n)\implies p(S(n))\\
	\end{align*}
	Then $p$ is true for all $n\in \N$.
\end{theorem}
\begin{proof}
	Consider the set $A$ defined as
	\begin{align*}
		A=\{n\in\N:p(n)\}
	\end{align*}
	By assumption $0\in A$ and for any $n\in A$, we have that
	$S(n)\in\N$ since \newline$p(n)\implies p(S(n))$. Therefore $A$
	may equivilantly be defined as
	\begin{align*}
		A=\{n\in\N:n=0\lor\exists m\in\N:S(m)=n\}
	\end{align*}
	But that is the definition of $\N$, so since $A$ may equivalently
	be defined as the set of $n\in\N$ such that $p(n)$ this completes
	the proof.
\end{proof}
\begin{definition}[Addition on $\N$]\label{addition}
	Let $+:\N^2\rightarrow\N$ we define \textit{addition} as
	follows
	\begin{align*}
	\forall n,m\in\N:
	\begin{cases}
		m+0 & = m \\
		m+S(n) &= S(m+n)
	\end{cases}
	\end{align*}
\end{definition}
\begin{theorem}
	Let $+$ be the operator as defined in definition~\ref{addition}. Then $+$
	satisfies
	\begin{align*}
		(G0)\; &:\quad \forall n,m\in\N &&:n+m\in\N 
		&&&\text{(Totality)}\\
		(G1)\; &:\quad \forall n,m,k\in\N&&:n+(m+k)=(n+m)+k
		&&&\text{(Associativity)}\\
		(G2)\; &:\quad \exists 0:\forall n\in\N &&: n+0=0+n=n 
		&&&\text{(Identity)}\\
		(C)\; &:\quad \forall n,m\in\N &&: n+m=m+n
		&&&\text{(Commutativity)}\\
	\end{align*}
\end{theorem}
\begin{proof}\
\begin{itemize}
	\item[] $(G0)\ $: This property follows from Theorem~\ref{nat
		proof}. Notice that in the case $m+0=m$, by definition
		$m+0\in\N$. Assume then for $n\in\N$ that $m+n\in\N$,
		then we have $m+S(n)=S(m+n)$ and since $m+n\in\N$,
		by Theorem~\ref{nat proof}. By Theorem~\ref{induction}
		and that $m$ was arbitrary this shows $(G0)$.
	\item[] $(G1)\ $: Notice that if any of $n,m,k\in\N$ this 
		property holds. So assume $n,m,k\neq 0$. This implies that
		for all of them there is some $n',m',k'\in\N$ such that
		$n=S(n')$ and equivalently for the others. So we may write
		\begin{align*}
			n+(m+k) &= S(n')+(m+k)\\
					&= S(n'+(m+k))\\
			(n+m)+k &= (n+m)+S(k')\\
					&= S((n+m)+k')\\
		\end{align*}
		So if $n'+(m+k)=(n+m)+k'$ this property follows. If
		$n'=0$ then
		\begin{align*}
			n'+(m+k)=m+k&=(n'+m)+k\\
						&=(n'+m)+S(k')\\
						&=S((n'+m)+k')\\
						&=S(n'+m)+k'\\
						&=n+m+k'\\
		\end{align*}
		the argument if $k'=0$ is similarly. If, on the other
		hand $k',n'\neq0$ repeat the process described earlier
		and note that eventually one of the must be equal to $0$.
	\item[] $(G2)\ $: This property follows trivially.
	\item[] $(C0)\ $:
\end{itemize}
\end{proof}
\begin{definition}[Order on $\N$]\label{order}
	We define an order on $\N$ as follows
	\begin{align*}
		\forall n,m\in\N:n\leq m:= \exists c\in\N:n+c=m
	\end{align*}
\end{definition}
\begin{theorem}[Order on $\N$ is total order]\label{well-ordered}
	Let $\leq$ be the order as defined in definition~\ref{order}. Then $\leq$
	satisfies
	\begin{align*}
		(0)\; &:\quad \forall n\in\N &&: n\leq n
		&&&\text{(Reflexive)}\\
		(1)\; &:\quad \forall n,m,k\in\N&&:n\leq m\land m\leq k\implies n\leq k
		&&&\text{(Transitive)}\\
		(2)\; &:\quad \forall n,m\in\N &&:n\leq m\land m\leq n \implies n=m
		&&&\text{(Antisymmetric)}\\
		(2)\; &:\quad \forall n,m\in\N &&:n\leq m\lor m\leq n
		&&&\text{(Connected)}\\
	\end{align*}
\end{theorem}
\begin{proof}
\end{proof}
\begin{theorem}[$\N$ is well-ordered]\label{well-ordered}
	Every subset of $N$ contains a least member.
	\begin{align*}
		\forall N\subseteq \N: \exists n\in N: \forall m\in\N: n\leq m
	\end{align*}
\end{theorem}
\begin{proof}
\end{proof}
\newpage
\begin{align*}
	&\forall a,b\in\R^n:S(x,y)=\prod_{i=1}^n [x_i,y_i]\\
	&\forall a,b\in\R^n:\mu (x,y)=\prod_{i=1}^n |x_i-y_i|\\
	&\forall X\subseteq\R^n: \lambda(X)
	=\inf \left\{\sum_{i=1}^n \mu(x,y):X\subseteq \bigcup_{i=1}^n S(x,y)\land
	n\in\N\right\}
\end{align*}
\section{Euclidean algebra}
Point: $A\in\R^2$.\newline
Distance: $A,B\in\R^2:|AB|:=\sqrt{{(A_1-B_1)}^2+{(A_2-B_2)}^2}$\newline
Line: $A,B\in\R^2: \overline{AB}:=\{tx+sy:t+s=1\land 0\leq t,s\leq 1\}$\newline
Circle: $A,B,O\in\R^2:c(A,B,):=\{C\in\R^2:|OC|=|AB|\}$\newline
Intersection: $A,B\in\R^2: A\mid B:= C:C$
\newpage
\begin{definition}[Gradient]
	Let $f:\R^n\rightarrow\R$, $\mathbf{x}\mapsto f(\mathbf{x})$ where:
\begin{align*}
	\mathbf{x}=
		\begin{bmatrix}
			x_1 \\
			x_2 \\
			\vdots \\
			x_n \\
		\end{bmatrix}
\end{align*}
Let $\frac{\del f}{\del x_i}$ exists for all $x_i$. The \textbf{gradient of $f$
(at $\mathbf{x}$)} is defined as the column matrix:
\begin{align*}
	\nabla f(\mathbf{x})=
		\begin{bmatrix}
			\frac{\del f(\mathbf{x})}{\del x_1} \\
			\frac{\del f(\mathbf{x})}{\del x_2} \\
			\vdots \\
			\frac{\del f(\mathbf{x})}{\del x_n} \\
		\end{bmatrix}
\end{align*}
\end{definition}
\begin{align*}
	\underset{i=k\rightarrow n}{[+a_i]} &= 
	\begin{cases}
		\underset{i=k\rightarrow n-1}{[+a_i]}+a_n & k\leq n\\
		0 & n < k
	\end{cases}
\end{align*}
\begin{align*}
	\underset{i=1\rightarrow n}{[+i]}=\frac{n(n+1)}{2}\\
	\underset{i=k\rightarrow n}{[+a_i]}+\underset{i=k\rightarrow n}{[+b_i]}
	=\underset{i=k\rightarrow n}{[+a_i+b_i]}\\
	\underset{i=k\rightarrow n}{[+a_i]}\cdot\underset{i=k\rightarrow n}{[+b_i]}=
	\underset{i=k\rightarrow n,\ i=k\rightarrow n}{[+a_i\cdot[+b_i]]}
\end{align*}
\newpage
\begin{enumerate}
	\item First postulate (principle of relativity)

		The laws of physics are the same in all inertial frames of reference.
	\item Second postulate (invariance of $c$)

		The speed of light in free space has the same value $c$
		in all inertial frames of reference.
\end{enumerate}
\begin{definition}[Space time]
	Space time is the Cartesian product $\R^3\times \mathbb{T}$ where
	$\mathbb{T}$ is the time domain $\mathbb{T}=\R$. That is any event may be
	indicated by its $xyz$-coordiantes and the time at which it occurred,
	formally:
	\begin{align*}
		(x,y,z,t)\in\R^3 \times \mathbb{T}\\
	\end{align*}
\end{definition}
\begin{definition}[Reference frame]
	A reference frame is a function $F$ such that for all $t\in\mathbb{T}$ we
	have $F(t)=(0,0,0)$. Intuitively this means that any object is distance zero
	from itself.
\end{definition}
\begin{definition}[World line]
	A world line is a function $W:\mathbb{T}\rightarrow \R^3$ such that if
	$W(t)=(x,y,z)$ then
	\begin{align*}
	v^2 t^2=x^2+y^2+z^2 \\
	\end{align*}
	where $v$ is the velocity of the object relative to the reference
	frame. This is equivalent to saying that the distance traveled
	from $t=0$ at speed $v$ equals the distance from the reference frame.
\end{definition}
Note that the reference frame is a special world line.
\begin{definition}[Transformation of reference frame]
	Given a world line $W$ there exists a transformation $L:
	\R^3\times\mathbb{T}\rightarrow\R^3\times\mathbb{T}$ 
	such that:
	\begin{enumerate}
		\item $L(W)$ is a reference frame.
		\item If world lines $W_1$ and $W_2$ have relative velocity $v$, then so
			do $L(W_1)$ and $L(W_2)$.
		\item For some constant $c\in\R_{>0}$ describe the world line $W$ with
			relative velocity $c$ to the reference frame $F$. Then the relative
			velocity to $L(W)$ is also $c$.
	\end{enumerate}
\end{definition}
\subsection{Special case}
\newpage
\begin{align*}
	a^2-a&=a(a-1)\\
	a^2-a&\equiv a(a-1) \pmod{2}\\
	a^2-a&\equiv 0 \pmod{2}\\
	a^2&\equiv a \pmod{2}\\
\end{align*}
\begin{align*}
	a,b,c\in\N: a^2+b^2&=4c+3\\
	a^2+b^2&\equiv 3 \pmod{4}\\
	a^2,b^2 &\in \{0,1,0,1\}
\end{align*}
\begin{align*}
	\forall n\in\N:\forall a\in\N: a^n &\equiv a \pmod{n}\\
\end{align*}
\begin{enumerate}
	\item Open-source, github like, paper writing.
	\item Issue tracker with monetary awards?
	\item Programming build in, jupyter like.
\end{enumerate}
\newpage
\begin{align*}
	a,b,c\in\N: a^2+b^2&=4c+3\\
	a^2+b^2&\equiv 3 \pmod{4}\\
	a^2,b^2 &\in \{0,1,0,1\}
\end{align*}
\begin{align*}
	&M = \{v_1,v_2,\dots,v_n\}\subset\R^2:\\
	&\rightarrow v_1+v_2+\cdots+v_n = \textbf{0} \\
	&\rightarrow \forall 1\leq i\leq n : \|v_i\|\leq 1\\
	&\rightarrow \forall 1\leq i\leq n : \|v_i\|\leq 1\\
\end{align*}
\begin{align*}
	\Z/2\Z\times\Z/3\Z = \{(0,0),(0,1),(0,2),(1,0),(1,1),(1,2)\}
\end{align*}
\begin{center}
\begin{tabular}{l|llllll}
	 + & 00 & 01 & 02 & 10 & 11 & 12 \\ \hline
	00 & 00 & 01 & 02 & 10 & 11 & 12 \\
	01 & 01 & 02 & 00 & 11 & 12 & 10 \\
	02 & 02 & 00 & 01 & 12 & 10 & 11 \\
	10 & 10 & 11 & 12 & 00 & 01 & 02 \\
	11 & 11 & 12 & 10 & 01 & 02 & 00 \\
	12 & 12 & 10 & 11 & 02 & 00 & 01 \\
\end{tabular}
\end{center}
\begin{center}
\begin{tabular}{l|llllll}
	  + & $e$ & $a$ & $b$ & $c$ & $d$ & $f$ \\ \hline
	$e$ & $e$ & $a$ & $b$ & $c$ & $d$ & $f$ \\
	$a$ & $a$ & $b$ & $e$ & $d$ & $f$ & $c$ \\
	$b$ & $b$ & $e$ & $a$ & $f$ & $c$ & $d$ \\
	$c$ & $c$ & $d$ & $f$ & $e$ & $a$ & $b$ \\
	$d$ & $d$ & $f$ & $c$ & $a$ & $b$ & $e$ \\
	$f$ & $f$ & $c$ & $d$ & $b$ & $e$ & $a$ \\
\end{tabular}
\end{center}
\begin{align*}
	\exp \int \ln x\ \dx x &= \exp\left(x\ln x -x\right)\\
						   &= \frac{x^x}{e^x}
\end{align*}
\newpage
\begin{align*}
	\text{GL}_n(\R) &= \{M\in\R^{n\times n}:\exists M^{-1}\in\R^{n\times
	n}:MM^{-1}=M^{-1}M=I_n\}\\
	\text{SL}_n(\R) &= \text{ker}(\text{det}),\; \text{det}:\text{GL}_n(\R)\rightarrow\R/\{0\}
\end{align*}
\begin{theorem}
	Let $G$ be a group and let $H,N\subseteq G$ such that $H\cap
	N=\emptyset$ and $H\cup N = G$. Then $H$ and $N$ are subgroups iff
	$H{\times}N \simeq G$.
\end{theorem}
\begin{align*}
	\prod_a^b f(x)^{\dx x} &= \exp \left(\int_a^b \ln f(x)\ \dx x\right) \\
	\prod_a^b {x}^{n\ \dx x} &= \exp \left(\int_a^b \ln x^n\ \dx x\right) \\
	&= \exp \left(\int_a^b n \ln x\ \dx x\right) \\
	&= \exp \left(\int_a^b \ln x\ \dx x\right)^n \\
	&= \exp \left(\left[x\ln x - x\right]_a^b\right)^n \\
	&= \exp \left(b\ln b - b - a\ln a + a\right)^n \\
	&= e^{nb\ln b}e^{-nb}e^{-na\ln a}e^{na} \\
	&= e^{n(a-b)}b^{nb}a^{-na} \\
	\prod_1^x {x}^{n\ \dx x} &= e^{n(1-x)}x^{nx} \\
	&= e^{n(1-x)}e^{nx\ln x} \\
	&= e^{n-nx+nx\ln x} \\
	&= e^n e^{n(x\ln x - x)} \\
\end{align*}
\begin{align*}
	f(z)&=\frac{1}{z}+z\\
		&=\frac{\bar{z}}{z\bar{z}}+z\\
		&=\frac{\bar{z}}{|z|^2}+z \\
		&=\frac{\bar{z}+|z|^2z}{|z|^2}\\
		&=\frac{\bar{z}+\bar{z}z^2}{|z|^2}
\end{align*}
\begin{align*}
	I &= \int_a^b \tan x\ \dx x \\
	  &= \int_a^b \frac{\sin x}{\cos x}\ \dx x \\
	u = \cos x &\implies \dx u = -\sin x\ \dx x\\
	  &= -\int_{\cos a}^{\cos b} \frac{\dx u}{u} \\
	  &= \ln\left(\frac{\cos a}{{\cos b}}\right) \\
\end{align*}
\begin{align}
	&ABCDEFGHIJKLMNOPQRSTUVWXYZ\\
	&\mathbf{ABCDEFGHIJKLMNOPQRSTUVWXYZ}\\
	&abcdefghijklmnopqrstuvwxyz\\
	&\mathbf{abcdefghijklmnopqrstuvwxyz}\\
	&\mathbb{ABCDEFGHIJKLMNOPQRSTUVWXYZ}\\
	&\mathcal{ABCDEFGHIJKLMNOPQRSTUVWXYZ}\\
	&\mathfrak{ABCDEFGHIJKLMNOPQRSTUVWXYZ}\\
\end{align}
\newpage
Let $A\in\M_{n{\times}m}(\F)$
\begin{align*}
	A =
		\begin{bmatrix}
			a_{11} & a_{12} & \cdots & a_{1m} \\
			a_{21} & a_{22} & \cdots & a_{2m} \\
			\vdots & \vdots & \ddots & \vdots \\
			a_{n1} & a_{n2} & \cdots & a_{nm} \\
		\end{bmatrix}
\end{align*}
\begin{align*}
	\mathbf{f}:\R^n\to\R^m \\
	\mathbf{x} = 
	\begin{bmatrix}
		x_1 \\
		x_2 \\
		\vdots \\
		x_n \\
	\end{bmatrix} \\
	\mathbf{f}(\mathbf{x}) = 
	\begin{bmatrix}
		f_1(\mathbf{x}) \\
		f_2(\mathbf{x}) \\
		\vdots \\
		f_m(\mathbf{x}) \\
	\end{bmatrix} \\
	\J_{(-)}(\mathbf{x}):\text{Hom}(\R^n,\R^m) \to \cL(\R^n,\R^m)\\
	{\J}_{\mathbf{f}}(\mathbf{x}) = 
		\begin{bmatrix}
			\frac{\del f_1}{\del x_1} & \frac{\del f_1}{\del x_2} & \cdots & \frac{\del f_1}{\del x_n} \\
			\frac{\del f_2}{\del x_1} & \frac{\del f_2}{\del x_2} & \cdots & \frac{\del f_2}{\del x_n} \\
			\vdots                    & \vdots                    & \ddots & \vdots                    \\
			\frac{\del f_m}{\del x_1} & \frac{\del f_m}{\del x_2} & \cdots & \frac{\del f_m}{\del x_n}
		\end{bmatrix} \\
		{{\J}_{\mathbf{f}}(\mathbf{x})}_{ij} = \frac{\del f_i}{\del x_j} =
		\del_{x_j} f_i \\
		\exists \mathbf{f}^{-1}(\mathbf{x}) \iff \det(\J_\mathbf{f}(\mathbf{x}))
			\neq 0
\end{align*}
\begin{align*}
	{\J_{\mathbf{f}+\mathbf{g}}(\mathbf{x})}_{ij} = \frac{\del (f_i+g_i)}{\del
	x_j} &= \frac{\del f_i}{\del{ x_j}}+\frac{\del g_i}{\del{ x_j}} =
	{\J_{\mathbf{f}}(\mathbf{x})}_{ij}+	{\J_{\mathbf{g}}(\mathbf{x})}_{ij} \\
	\implies
	{\J_{\mathbf{f}+\mathbf{g}}(\mathbf{x})} &=
	{\J_{\mathbf{f}}(\mathbf{x})}+{\J_{\mathbf{g}}(\mathbf{x})} \\
	\\
	{\J_{\lambda\mathbf{f}}(\mathbf{x})}_{ij} = \frac{\del (\lambda f_i)}{\del
	x_j} &= \lambda \frac{\del f_i}{\del x_j} =
	\lambda {\J_{\mathbf{f}}(\mathbf{x})}_{ij} \\
	\implies {\J_{\lambda\mathbf{f}}(\mathbf{x})} &=
	\lambda {\J_{\mathbf{f}}(\mathbf{x})} \\
	\\
\end{align*}
\begin{align}
	a+bi = \begin{bmatrix} a & -b \\ b & a \end{bmatrix}
\end{align}
\begin{align*}
	(x+yi)^2 &= x^2+2ixy-y^2\\
	u(x,y) &= x^2-y^2 \\
	v(x,y) &= 2xy \\
	\frac{\del u}{\del x} = 2x &= \frac{\del v}{\del y} \\
	\frac{\del u}{\del y} = -2y &= -(2y) = -\frac{\del v}{\del x}\\
	\\
	\J_{\mathbf{f}}(x,y) &=
	\begin{bmatrix}
		2x & -2y \\
		2y & 2x
	\end{bmatrix}
	= 2x+2iy \\
	\\
	\frac{\del (f_i\circ g_i)}{\del x_j} &= \frac{\del (f_i\circ g_i)}{\del
	g_i}\frac{\del g_i}{\del x_j}
\end{align*}
\begin{align*}
	\grad f = \nabla f &=\frac{\del f}{\del x}\mathbf{i}
	+\frac{\del f}{\del y}\mathbf{j}
	+\frac{\del f}{\del z}\mathbf{k} \\
	\dive \mathbf{f} = \nabla \cdot \mathbf{f} &= \frac{\del f_x}{\del x} + \frac{\del
	f_y}{\del y} + \frac{\del f_z}{\del z} \\
	\curl\mathbf{f} = \nabla \times \mathbf{f} &=
	\left(\frac{\del f_z}{\del y}-\frac{\del f_y}{\del z}\right)\mathbf{i}
	+\left(\frac{\del f_x}{\del z}-\frac{\del f_z}{\del x}\right)\mathbf{j}
	+\left(\frac{\del f_y}{\del x}-\frac{\del f_x}{\del y}\right)\mathbf{k}\\
	\nabla^2 f &=
	\frac{\del^2 f}{\del^2 x} +
	\frac{\del^2 f}{\del^2 y} +
	\frac{\del^2 f}{\del^2 z}
\end{align*}
\begin{align*}
	\begin{bmatrix}
		\frac{\del}{\del x} \\
		\frac{\del}{\del y} \\
		\frac{\del}{\del z} \\
	\end{bmatrix}
	{\times}
	\begin{bmatrix}
		f(x) \\
		f(x) \\
		f(x) \\
	\end{bmatrix}
	=
	\begin{bmatrix}
		\frac{\del f(\mathbf{x})}{\del y}-\frac{\del f(\mathbf{x})}{\del z} \\
		\frac{\del f(\mathbf{x})}{\del z}-\frac{\del f(\mathbf{x})}{\del x} \\
		\frac{\del f(\mathbf{x})}{\del x}-\frac{\del f(\mathbf{x})}{\del y} \\
	\end{bmatrix}
\end{align*}
\begin{align*}
	\begin{bmatrix}
		x_1 \\
		x_2 \\
		x_3 \\
	\end{bmatrix}
	{\times}
	\begin{bmatrix}
		y_1 \\
		y_2 \\
		y_3 \\
	\end{bmatrix}
	=
	\begin{bmatrix}
		x_2y_3-x_3y_2 \\
		x_3y_1-x_1y_3 \\
		x_1y_2-x_2y_1 \\
	\end{bmatrix} \\
\end{align*}
\begin{align*}
	\times:\R^3\times\R^3&\to\R^3\\
	\ihat\times\jhat = \khat,\ \jhat\times\khat &= \ihat,\
	\khat\times\ihat = \jhat \\
	\mathbf{x}\times\mathbf{y} &= -\mathbf{y}\times\mathbf{x} \\
	(\lambda\mathbf{x})\times\mathbf{y} &= \lambda(\mathbf{x}\times\mathbf{y})\\
	(\mathbf{x}+\mathbf{y})\times\mathbf{z} &=
	\mathbf{x}\times\mathbf{z}+\mathbf{y}\times\mathbf{z}\\
	\\
	\mathbf{x}\times(\mathbf{y}+\mathbf{z}) &=
	-(\mathbf{y}+\mathbf{z})\times\mathbf{x} \\
	&=-\mathbf{y}\times\mathbf{x}-\mathbf{z}\times\mathbf{x} \\
	&=\mathbf{x}\times\mathbf{y}+\mathbf{x}\times\mathbf{z} \\
	\\
	\mathbf{x}\times(\lambda\mathbf{y})&= -(\lambda\mathbf{y})\times\mathbf{x}\\
									   &= -\lambda(\mathbf{y}\times\mathbf{x})\\
									   &= \lambda(\mathbf{x}\times\mathbf{y}) \\
	\\
	\mathbf{x}\times\mathbf{x} &= -\mathbf{x}\times\mathbf{x} \\
	\mathbf{x}\times\mathbf{x} &= \mathbf{0} \\
	\\
	x_1\ihat\times(y_1\ihat+y_2\jhat+y_3\khat)
	&= x_1y_1(\ihat\times\ihat)+x_1y_2(\ihat\times\jhat)+x_1y_3(\ihat\times\khat) \\
	&= x_1y_2\khat-x_1y_3\jhat \\
	x_2\jhat\times(y_1\ihat+y_2\jhat+y_3\khat) &= 
	x_2y_1(\jhat\times\ihat)+x_2y_2(\jhat\times\jhat)+x_2y_3(\jhat\times\khat)\\
	&= x_2y_3\ihat-x_2y_1\khat\\
	x_3\khat\times(y_1\ihat+y_2\jhat+y_3\khat) &= 
	x_3y_1(\khat\times\ihat)+x_3y_2(\khat\times\jhat)+x_3y_3(\khat\times\khat)\\
	&= x_3y_1\jhat-x_3y_2\ihat\\
	\\
	\begin{bmatrix}
		x_1 \\
		x_2 \\
		x_3 \\
	\end{bmatrix}
	{\times}
	\begin{bmatrix}
		y_1 \\
		y_2 \\
		y_3 \\
	\end{bmatrix}
	&=
	\begin{bmatrix}
		x_2y_3-x_3y_2 \\
		x_3y_1-x_1y_3 \\
		x_1y_2-x_2y_1 \\
	\end{bmatrix} \\
\end{align*}
\begin{align*}
	\cdot:\R^n\times\R^n&\to\R\\
	\mathbf{e}_i\cdot \mathbf{e}_j &= {\delta}_{ij} \\
	\mathbf{x}\cdot\mathbf{y} &= \mathbf{y}\cdot\mathbf{x} \\
	(\lambda\mathbf{x})\cdot\mathbf{y} &= \lambda(\mathbf{x}\cdot\mathbf{y})\\
	(\mathbf{x}+\mathbf{y})\cdot\mathbf{z} &=
	\mathbf{x}\cdot\mathbf{z}+\mathbf{y}\cdot\mathbf{z}\\
	\\
	\mathbf{x}\cdot(\mathbf{y}+\mathbf{z})
	&=(\mathbf{y}+\mathbf{z})\cdot\mathbf{x} \\
	&=\mathbf{y}\cdot\mathbf{x}+\mathbf{z}\cdot\mathbf{x} \\
	&=\mathbf{x}\cdot\mathbf{y}+\mathbf{x}\cdot\mathbf{z} \\
	\\
	\mathbf{x}\cdot(\lambda\mathbf{y})
	&= (\lambda\mathbf{y})\cdot\mathbf{x} \\
	&= \lambda(\mathbf{y}\cdot\mathbf{x}) \\
	&= \lambda(\mathbf{x}\cdot\mathbf{y}) \\
	\\
	\mathbf{x}\cdot\mathbf{y} 
	&= \left(\sum_{i=1}^n x_i\mathbf{e}_i\right)\cdot
	\left(\sum_{k=1}^n y_k\mathbf{e}_k\right) \\
	&= \sum_{i=1}^n \sum_{k=1}^n x_i\mathbf{e}_i\cdot y_k\mathbf{e}_k \\
	&= \sum_{i=1}^n \sum_{k=1}^n x_i y_k (\mathbf{e}_i\cdot \mathbf{e}_k) \\
	&= \sum_{i=1}^n \sum_{k=1}^n {\delta}_{ik} x_i {y}_{k}  \\
	&= \sum_{i=1}^n x_i y_i  \\
\end{align*}
\newpage
\begin{definition}[Del Operator]
	Let $\mathbf{V}$ be a vector space of dimension $n$. Let
	$(\mathbf{e}_1,\mathbf{e}_2,\dots,\mathbf{e}_n)$ be the standard ordered
	basis of $\mathbf{V}$, that is $(\mathbf{e}_i)_j = \delta_{ij}$. The
	\textit{del operator} is a unary operator on $\mathbf{V}$ defined as:
	\begin{align*}
		\nabla := \sum_{j=1}^{n} \mathbf{e}_j \frac{\del }{\del x_j}
	\end{align*}
	Where $\mathbf{x} = \sum_{j=1}^{n} x_j\mathbf{e}_j$ is an arbitary
	element of $\mathbf{V}$.
\end{definition}
\begin{align*}
	\nabla\cdot\mathbf{E} &= \frac{\rho}{\varepsilon_0} \\
	\nabla\cdot\mathbf{B} &= 0 \\
	\nabla\times\mathbf{E} &= -\frac{\del\mathbf{B}}{\del t} \\
	\nabla\times\mathbf{B} &= -\mu_0\left(\mathbf{J}+\varepsilon_0\frac{\del
	\mathbf{E}}{\del t}\right) \\
\end{align*}
\begin{align*}
	{R}_{\mu \nu} - \frac{1}{2} R {g}_{\mu \nu} + \Lambda {g}_{\mu \nu}
	= \frac{8 \pi G}{c^4} {T}_{\mu \nu}
\end{align*}
\begin{align*}
	i\hbar\frac{\del}{\del t} |\Psi(t)\rangle = \hat{H}|\Psi(t)\rangle
\end{align*}
Let a disk with center $O$ and radius $r$ have angular velocity $\omega$.
Let $P$ be a point along it's circumference and consider an observer at $O$. The
relative velocity along the radius $r$ between $O$ and $P$ is clearly 0. By the
definition of angular velocity the relative velocity tangent to the
circumference is $u=\omega r$. Consider now a small step along the circumference
$\dx L$. Due to length contraction we have $\dx L =\dx L_0}/\gamma$ where
$\gamma = 1/\sqrt{1-u^2/c^2}$ and $\dx L_0$ is the length of the step when
$\omega = 0$ in which case geometry is euclidean and thus $\dx L_0 = r\ \dx \theta $
for some correspondingly small angle $\dx \theta$. Thus the circumference of the
disk is:
\begin{align*}
	C = \int_{0}^{C} \dx L = \int_{0}^{C_0} \frac{\dx L_0}{\gamma} =
	\int_{0}^{2\pi} \frac{r\ \dx \theta}{\gamma}  = \frac{2\pi r}{\gamma}
\end{align*}
Thus the geometry of space is non-euclidean.
\begin{align*}
	{\eta}_{\mu\nu} &=
	\begin{pmatrix}
		-1 & 0 & 0 & 0 \\
		0  & 1 & 0 & 0 \\
		0  & 0 & 1 & 0 \\
		0  & 0 & 0 & 1 \\
	\end{pmatrix}
	\\
	{\eta}_{\mu\nu}{p}^{\mu}q^{\nu} &= \frac{E_pE_q}{c^2}-p_x q_x-p_y q_y - p_z
	q_z \\
	{\eta}_{\mu\nu} p^{\mu} p^{\nu} &= m_0^2c^2
\end{align*}
\newpage
\begin{align*}
	\mathbf{f}:\R\to\R^2,\; \mathbf{f}=\mathbf{e}_{i} f_i\text{ so }
	\dx^2\mathbf{f} =\mathbf{e}_{i}\ \dx^2 f_i \text{ exists, where } i=1,2 \\
	\text{Let }n^1 = \frac{\mathbf{e}_i\ \dx f_i}{\sqrt{f_j'f_j'}},\;\;
	n^2 = \frac{\varepsilon_{ik}\mathbf{e}_k\ \dx f_i}{\sqrt{f_j'f_j'}}\text{ we
	clearly have } n^1_in^1_i =n^2_in^2_i = 1\text{ and } n^1_in^2_i = 0 \\
	\text{Then } n^{j}_{i} \in \R^{2\times 2}\text{ thus } \det(n^j_i) 
	= n^1_1 n^2_2 - n^1_2 n^2_1 =
	\frac{f_1'f_1'}{f_j'f_j'}-\frac{-f_2'f_2'}{f_j'f_j'} = 1\\
\end{align*}
\begin{align*}
	\nabla &= \mathbf{e}_j\frac{\del}{\del x_j} \\
\end{align*}
\begin{align*}
	\frac{a}{b+c}+\frac{b}{a+c}+\frac{c}{a+b} &= 4 \\
	a(a+c)(a+b) &= 4(b+c)(a+c)(a+b) \\
	+b(b+c)(a+b)& \\
	+c(b+c)(a+c)& \\
	a(a^2+ab+ac+bc) &= 4(b+c)(a^2+ab+ac+bc) \\
	+b(ab+b^2+ac+bc)& \\
	+c(ab+bc+ac+c^2)& \\
	a^3+a^2b+a^2c+abc &= 4(a^2b+a^2c+ab^2+abc+abc+ac^2+b^2c+bc^2) \\
	+ab^2+b^3+abc+b^2c&\\
	+abc+bc^2+ac^2+c^3&\\
	a^3+a^2b+a^2c &= 4a^2b+4a^2c+4ab^2+8abc+4ac^2+4b^2c+4bc^2 \\
	+ab^2+b^3+b^2c&\\
	+bc^2+ac^2+c^3&\\
			 +3abc&\\
	a^3+b^3+c^3 &= 3a^2b+3a^2c+3ab^2+3b^2c+3ac^2+3bc^2+5abc \\
\end{align*}
\begin{align*}
	\mathbf{x}=x_i\mathbf{e}_i \implies \mathbf{e}_i = \frac{\del
	\mathbf{x}}{\del x_i} \\
\end{align*}
\newpage
\begin{align*}
	\dx s^2 &= \dx x^2+\dx y^2 \\
	\frac{\dx s^2}{\dx x^2} &= 1+{\left(\frac{\dx y}{\dx x}\right)}^2 \\
	\frac{\dx s}{\dx x} &= \sqrt{1+{\left(\frac{\dx y}{\dx x}\right)}^2} \\
	\int_a^b \frac{\dx s}{\dx x} \dx x &= \int_a^b\sqrt{1+{\left(\frac{\dx
	y}{\dx x}\right)}^2}\ \dx x \\
	\int_{s(a)}^{s(b)} \dx s &= \int_a^b\sqrt{1+{\left(\frac{\dx y}{\dx
	x}\right)}^2}\ \dx x \\
	s(b)-s(a) &= \int_a^b\sqrt{1+{\left(\frac{\dx y}{\dx x}\right)}^2}\ \dx x
\end{align*}
\begin{align*}
	\dx s^2 &= \dx x^2+\dx y^2 \\
	\frac{\dx s^2}{\dx t^2} &= \frac{\dx x^2}{\dx t^2} + \frac{\dx y^2}{\dx t^2} \\
	\frac{\dx s}{\dx t} &= \sqrt{\left(\frac{\dx x}{\dx t}\right)^2 + {\left(\frac{\dx y}{\dx t}\right)}^2} \\
	\int \frac{\dx s}{\dx t}\ \dx t &= \int 
	\sqrt{\left(\frac{\dx x}{\dx t}\right)^2 + {\left(\frac{\dx y}{\dx
	t}\right)}^2}\ \dx t \\
	\int \frac{\dx s}{\dx t}\ \dx t &= \int \|y'(t)\|\ \dx t \\
	s &= \int \|y'(t)\|\ \dx t \\
\end{align*}
\begin{align*}
	\oint_C f(z)\ \dx z &= \sum_{i=1}^n \int_{a_i}^{b_i} f(\gamma_i(t))\gamma_i '(t)\ \dx t \\
\end{align*}
\begin{align*}
	h^A(-) &= \text{Hom}_{\mathcal{C}}(A,-) \\
	\text{Nat}(h^A,F) &\cong F(A) \\
	\text{Nat}(h^A,h^B) &\cong h^B(A) \cong \text{Hom}_{\mathcal{C}}(B,A)\\
\end{align*}
\newpage
\begin{align*}
	\mathbf{e}_i\wedge\mathbf{e}_i &= 0 \\
	\mathbf{x}\wedge\mathbf{y} &= -\mathbf{y}\wedge\mathbf{x} \\
	(\lambda\mathbf{x})\wedge\mathbf{y} &= \lambda(\mathbf{x}\wedge\mathbf{y}) \\
	(\mathbf{x}+\mathbf{y})\wedge\mathbf{z}
	&= \mathbf{x}\wedge\mathbf{z}+\mathbf{x}\wedge\mathbf{z} \\
	\\
	\mathbf{e}_i\cdot \mathbf{e}_j &= {\delta}_{ij} \\
	\mathbf{x}\cdot\mathbf{y} &= \mathbf{y}\cdot\mathbf{x} \\
	(\lambda\mathbf{x})\cdot\mathbf{y} &= \lambda(\mathbf{x}\cdot\mathbf{y})\\
	(\mathbf{x}+\mathbf{y})\cdot\mathbf{z} &=
	\mathbf{x}\cdot\mathbf{z}+\mathbf{y}\cdot\mathbf{z}\\
	\\
	\mathbf{x}\mathbf{y} &= \mathbf{x}\cdot\mathbf{y}+\mathbf{x}\wedge\mathbf{y}
	\\
	\mathbf{e}_1\mathbf{e}_2 &=  \mathbf{e}_1\wedge\mathbf{e}_2 \\
	(\mathbf{e}_1\mathbf{e}_2)^2
	&= \mathbf{e}_1\mathbf{e}_2\mathbf{e}_1\mathbf{e}_2 \\
	&=-\mathbf{e}_1\mathbf{e}_1\mathbf{e}_2\mathbf{e}_2 \\
	&=-1 \\
\end{align*}
\newpage
\begin{align*}
	|x-x_0| &\leq \min\left(1,\frac{\varepsilon}{2|y_0|}\right) \\
	|y-y_0| &\leq \frac{\varepsilon}{2(1+|x_0|)} \\
	\\
	|x|-|x_0| &\leq |x-x_0| \leq 1 \\
	|x| &\leq  1+|x_0| \\
	\\
	|xy-x_0y_0|&= |y_0x-y_0x_0+xy-xy_0|\\
	&= |y_0(x-x_0)+x(y-y_0)|\\
	&\leq |y_0(x-x_0)|+|x(y-y_0)|\\
	&\leq |y_0||x-x_0|+|x||y-y_0|\\
	&\leq |y_0||x-x_0|+(1+|x_0|)|y-y_0|\\
	&\leq |y_0|\frac{\varepsilon}{2|y_0|}
	      +(1+|x_0|)\frac{\varepsilon}{2(1+|x_0|)}\\
	&\leq \frac{\varepsilon}{2}+\frac{\varepsilon}{2}=\varepsilon\\
\end{align*}
\begin{align*}
	\left|\frac{1}{y}-\frac{1}{y_0}\right| &=
	\left|\frac{y_0-y}{yy_0}\right| \\
	&= \frac{|y_0-y|}{|yy_0|} \\
	&\leq \frac{\varepsilon|yy_0|}{|yy_0|} \\
\end{align*}
\newpage
\begin{align*}
	\beta = \frac{v}{c},\; \gamma &= \frac{1}{\sqrt{1-\beta^2}} \\
	\Lambda &\equiv
	\begin{pmatrix}
		\tensor{\Lambda}{^0_0} & \tensor{\Lambda}{^0_1} & \tensor{\Lambda}{^0_2} & \tensor{\Lambda}{^0_3} \\
		\tensor{\Lambda}{^1_0} & \tensor{\Lambda}{^1_1} & \tensor{\Lambda}{^1_2} & \tensor{\Lambda}{^1_3} \\
		\tensor{\Lambda}{^2_0} & \tensor{\Lambda}{^2_1} & \tensor{\Lambda}{^2_2} & \tensor{\Lambda}{^2_3} \\
		\tensor{\Lambda}{^3_0} & \tensor{\Lambda}{^3_1} & \tensor{\Lambda}{^3_2} & \tensor{\Lambda}{^3_3} \\
	\end{pmatrix} \\
	\eta &\equiv
	\begin{pmatrix}
		1 & 0 & 0 & 0 \\
		0 & -1 & 0 & 0 \\
		0 & 0 & -1 & 0 \\
		0 & 0 & 0 & -1 \\
	\end{pmatrix} \\
	\tensor{(\Lambda^{-1})}{^\mu_\beta}\tensor{\Lambda}{^\beta_\nu} &=
	\tensor{\delta}{^\mu_\nu},\\ 
	\tensor{(\Lambda^{-1})}{^\mu_\nu}&\equiv\tensor{\Lambda}{_\mu^\nu} \\
	\tensor{A}{_\mu} &\equiv \tensor{\eta}{_{\mu\nu}}\tensor{A}{^\nu} \\
	\tensor{A'}{^\mu} &\equiv \tensor{\Lambda}{^\mu_\nu}\tensor{A}{^\nu} \\
	\tensor{\eta}{_{\mu\nu}} &=
	\tensor{\Lambda}{^\alpha_\mu}
	\tensor{\Lambda}{^\beta_\nu}
	\tensor{\eta}{_{\alpha\beta}}
	\\
	\\
	\tensor{\Lambda}{^\alpha_\mu}
	\tensor{\Lambda}{^\beta_\nu}
	\tensor{\eta}{_\alpha_\beta}
	&= \tensor{\eta}{_\mu_\nu} \\
	\tensor{\Lambda}{^\alpha_\mu}
	\tensor{\eta}{^\alpha_\beta}
	\tensor{\Lambda}{^\beta_\nu}
	&= \tensor{\eta}{^\mu_\nu} \\
	\tensor{(\Lambda^\top)}{^\mu_\alpha} 
	\tensor{(\eta\Lambda)}{^\alpha_\nu} 
	&= \tensor{\eta}{^\mu_\nu} \\
	\tensor{(\Lambda^\top\eta\Lambda)}{^\mu_\nu} &= \tensor{\eta}{^\mu_\nu} \\
	\Lambda^\top\eta\Lambda &= \eta \\
	\\
	A\cdot B &= \tensor{A}{_\alpha}\tensor{B}{^\alpha} =
	\tensor{\eta}{_{\alpha\beta}}\tensor{A}{^\alpha}\tensor{B}{^\beta}\\
	A'\cdot B' &= \tensor{A'}{_\mu}\tensor{B'}{^\mu} \\
	&= \tensor{\eta}{_\mu_\nu}\tensor{A'}{^\nu}\tensor{B'}{^\mu} \\
	&= \tensor{\eta}{_\mu_\nu}
	\tensor{\Lambda}{^\nu_\alpha}\tensor{A}{^\alpha}
	\tensor{\Lambda}{^\mu_\beta}\tensor{B}{^\beta}
	\\
	&= 
	\tensor{\Lambda}{^\nu_\alpha}\tensor{\Lambda}{^\mu_\beta}
	\tensor{\eta}{_\mu_\nu}
	\tensor{A}{^\alpha}\tensor{B}{^\beta}
	\\
	&= \tensor{\eta}{_\alpha_\beta}
	\tensor{A}{^\alpha}\tensor{B}{^\beta}\\
	\\
\end{align*}
\newpage
\begin{align*}
	\eta &= \Lambda^\top\eta\Lambda \\
	\eta\Lambda^{-1} &= \Lambda^\top\eta \\
	\Lambda^{-1} &= \eta\Lambda^\top\eta \\
	\\
	\tensor{\Lambda}{_\mu^\beta} 
	\tensor{\Lambda}{^\mu_\nu} 
	&=
	\tensor{\delta}{^\beta_\nu}
	\\
	\tensor{(\Lambda\Lambda^{-1})}{^\mu_\nu}
	&=
	\tensor{\delta}{^\mu_\nu}
	\\
	\tensor{\Lambda}{^\mu_\alpha}
	\tensor{(\Lambda^{-1})}{^\alpha_\nu}
	&=
	\tensor{\delta}{^\mu_\nu}
	\\
	\tensor{\Lambda}{^\mu_\alpha}
	\tensor{\Lambda^{-1}}{_\nu^\alpha}
	&=
	\tensor{\delta}{^\mu_\nu}
	\\
	\\
	\tensor{\eta}{_{\mu\nu}} &= 
	\tensor{\Lambda}{^\alpha_\mu}
	\tensor{\Lambda}{^\beta_\nu}
	\tensor{\eta}{_{\alpha\beta}} \\
	\tensor{\eta}{_{\mu\nu}}
	\tensor{\Lambda}{_\sigma^\nu} 
	&=
	\tensor{\Lambda}{^\alpha_\mu}
	\tensor{\eta}{_{\alpha\beta}}
	\tensor{\Lambda}{^\beta_\nu} 
	\tensor{\Lambda}{_\sigma^\nu} 
	\\
	\tensor{\eta}{_{\mu\nu}}
	\tensor{\Lambda}{_\sigma^\nu} 
	&=
	\tensor{\Lambda}{^\alpha_\mu}
	\tensor{\eta}{_{\alpha\beta}}
	\tensor{\delta}{^\beta_\sigma} 
	\\
	\tensor{\eta}{_{\mu\nu}}
	\tensor{\Lambda}{_\beta^\nu} 
	&=
	\tensor{\Lambda}{^\alpha_\mu}
	\tensor{\eta}{_{\alpha\beta}}
	\\
	\tensor{\eta}{^{\sigma\mu}}
	\tensor{\eta}{_{\mu\nu}}
	\tensor{\Lambda}{_\beta^\nu} 
	&=
	\tensor{\eta}{^{\sigma\mu}}
	\tensor{\Lambda}{^\alpha_\mu}
	\tensor{\eta}{_{\alpha\beta}}
	\\
	\tensor{\delta}{^\sigma_\nu}
	\tensor{\Lambda}{_\beta^\nu} 
	&=
	\tensor{\eta}{_{\sigma\mu}}
	\tensor{\Lambda}{^\alpha_\mu}
	\tensor{\eta}{_{\alpha\beta}}
	\\
	\tensor{\Lambda}{_\beta^\nu} 
	&=
	\tensor{\eta}{_{\nu\mu}}
	\tensor{\Lambda}{^\alpha_\mu}
	\tensor{\eta}{_{\alpha\beta}}
	\\
	\\
	\det(\eta) &= \det(\Lambda^\top\eta\Lambda) \\
	-1 &= -\det(\Lambda^\top)\det(\Lambda) \\
	1 &= \det(\Lambda)^2 \\
	\det(\Lambda)^2 &= \pm 1
\end{align*}
\end{document}
